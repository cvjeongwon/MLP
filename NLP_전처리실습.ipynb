{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMMqkol8ZNzgTY0SlCkr3/w",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cvjeongwon/MLP/blob/main/NLP_%EC%A0%84%EC%B2%98%EB%A6%AC%EC%8B%A4%EC%8A%B5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wz6XHQ6-2RBF",
        "outputId": "7f606cfd-2580-4d8d-e3a2-ee1723efcf10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Model-based', 'RL', 'do', \"n't\", 'need', 'a', 'value', 'function', 'for', 'the', 'ploicy', '.']\n"
          ]
        }
      ],
      "source": [
        "# 1. 표준 토큰화\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "\n",
        "tokenizer = TreebankWordTokenizer()\n",
        "text = \"Model-based RL don't need a value function for the ploicy.\"\n",
        "print(tokenizer.tokenize(text))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# \n",
        "from nltk.tokenize import word_tokenize\n",
        "# print(word_tokenize(text))"
      ],
      "metadata": {
        "id": "fqYQ5rLQ2jL8"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "from nltk.stem import PorterStemmer, LancasterStemmer\n",
        "stem1 = PorterStemmer()\n",
        "stem2 = LancasterStemmer()\n",
        "\n",
        "words=[\"eat\", \"ate\",\"eaten\", \"eating\"]\n",
        "print(\"PoterStemmer    :\", [stem1.stem(w) for w in words])\n",
        "print(\"LancasterStemmer:\", [stem2.stem(w) for w in words])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pIzSbQHw3r1Z",
        "outputId": "552d5138-569e-49ca-8dea-93d39f1d1cc0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PoterStemmer    : ['eat', 'ate', 'eaten', 'eat']\n",
            "LancasterStemmer: ['eat', 'at', 'eat', 'eat']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "lemm = WordNetLemmatizer()\n",
        "words=[\"eat\", \"ate\",\"eaten\", \"eating\"]\n",
        "print(\"WordNet Lemmatizer: \" ,[lemm.lemmatize(w,pos=\"v\")for w in words])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2G5v8XL4xIJ",
        "outputId": "393b386f-f7b7-4e6b-e72c-dc50a9e0bc73"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WordNet Lemmatizer:  ['eat', 'eat', 'eat', 'eat']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. StopWords 불용어\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "print(stopwords.words('english')[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jgVf1lFs5Qf5",
        "outputId": "a2d0f07b-b493-495a-a9c3-f8f745c1a3b0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install JPype1"
      ],
      "metadata": {
        "id": "Sl8JzvTl7CkT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install konlpy"
      ],
      "metadata": {
        "id": "T-c6hsKs7S0L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 토큰화 후 한글 불용어 제거하기\n",
        "# from konlpy.corpus import kobill\n",
        "from konlpy.tag import Okt\n",
        "\n",
        "okt = Okt()\n",
        "\n",
        "text =\"고기를 아무렇게나 구우려고 하면 안 돼. 고기라고 다 같은 게 아니거든. 예컨대 삼겹살을 구울 때는 중요한 게 있지.\"\n",
        "stop_words = \"를 아무렇게나 구 우려 고 안 돼 같은 게 구울 때 는\"\n",
        "\n",
        "stop_words = set(stop_words.split(' '))\n",
        "word_tokens = okt.morphs(text)\n",
        "result =[word for word in word_tokens if not word in stop_words]\n",
        "\n",
        "print(\"불용어 제거 전:\", word_tokens)\n",
        "print(\"불용어 제거 후:\", result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "estul1eq6HQZ",
        "outputId": "a76efa98-a1c8-4636-8ce5-a3d9a26ff1a8"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "불용어 제거 전: ['고기', '를', '아무렇게나', '구', '우려', '고', '하면', '안', '돼', '.', '고기', '라고', '다', '같은', '게', '아니거든', '.', '예컨대', '삼겹살', '을', '구울', '때', '는', '중요한', '게', '있지', '.']\n",
            "불용어 제거 후: ['고기', '하면', '.', '고기', '라고', '다', '아니거든', '.', '예컨대', '삼겹살', '을', '중요한', '있지', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Enumerate 예제\n",
        "mylist =['Apple', 'Tomato', 'Buleberry']\n",
        "for n, name in enumerate(mylist):\n",
        "  print(\"Fruit: {}, Number: {}\".format(name, n))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hgLDvd-E6sN5",
        "outputId": "86a48d47-9977-4b97-c5a0-b7716997b134"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fruit: Apple, Number: 0\n",
            "Fruit: Tomato, Number: 1\n",
            "Fruit: Buleberry, Number: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. 정수 인코딩 & Sorting\n",
        "vocab ={'apple':2, 'Tomato':6, 'Rice': 4, 'Blueberry': 1}\n",
        "vocab_sort = sorted(vocab.items(), key=lambda x: x[1], reverse=True)\n",
        "print(vocab_sort)\n",
        "word2inx = {word[0]: index+1 for index, word in enumerate(vocab_sort)}\n",
        "print(word2inx)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dM8OICJq-8EP",
        "outputId": "2f4121ce-6ec1-46cd-8771-a966a2304528"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Tomato', 6), ('Rice', 4), ('apple', 2), ('Blueberry', 1)]\n",
            "{'Tomato': 1, 'Rice': 2, 'apple': 3, 'Blueberry': 4}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. 코사인 Similarity\n",
        "import numpy as np\n",
        "def cos_sim(A,B):\n",
        "  return np.dot(A,B) / (np.linalg.norm(A)*np.linalg.norm(B))\n",
        "\n",
        "a = [1,0,0,1]\n",
        "b = [0,1,1,0]\n",
        "c = [1,1,1,1]\n",
        "print(cos_sim(a,b), cos_sim(b,c), cos_sim(c,a))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yYSXquuy_l8H",
        "outputId": "f4410edb-7486-44a9-ed41-4d9cf196b22e"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0 0.7071067811865475 0.7071067811865475\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Levenshtein distance](https://en.wikipedia.org/wiki/Levenshtein_distance)"
      ],
      "metadata": {
        "id": "mdnsRtqhDv8w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# text1과 text2 간의 레벤슈타인 거리 계산 함수\n",
        "def leven(text1, text2):\n",
        "    len1 = len(text1) + 1  # text1의 길이 + 1\n",
        "    len2 = len(text2) + 1  # text2의 길이 + 1\n",
        "    sim_array = np.zeros((len1, len2))  # 2차원 배열 초기화\n",
        "    sim_array[:,0] = np.linspace(0, len1-1, len1)  # 첫 번째 열 초기화\n",
        "    sim_array[0,:] = np.linspace(0, len2-1, len2)  # 첫 번째 행 초기화\n",
        "    for i in range(1,len1):\n",
        "        for j in range(1,len2):\n",
        "            add_char = sim_array[i-1,j] + 1  # 추가 연산 횟수\n",
        "            sub_char = sim_array[i,j-1] + 1  # 삭제 연산 횟수\n",
        "            if text1[i-1] == text2[j-1]:\n",
        "                mod_char = sim_array[i-1,j-1]  # 수정 연산 (동일한 문자), mod_char:이전 위치의 연산횟수\n",
        "            else:\n",
        "                mod_char = sim_array[i-1,j-1] + 1  # 수정 연산 (다른 문자), 이전위치에서의 연산횟수+1\n",
        "            sim_array[i,j] = min([add_char, sub_char, mod_char])  # 가장 작은 연산 횟수 선택\n",
        "    return sim_array[-1,-1]  # 최종 레벤슈타인 거리 반환, 2차원 배열의 마지막 요소"
      ],
      "metadata": {
        "id": "jNb6-GIaD0cF"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(leven('머신러닝','머신너닝')) # 최소수정횟수가 1\n",
        "print(leven('자연어 처리','자연어쩌리연산'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aXDv9sgFGDGI",
        "outputId": "35400e8f-c2a5-47c0-d012-334ec23f4815"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.0\n",
            "4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iSzq7TrwQN9J"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}