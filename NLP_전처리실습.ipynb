{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1RPRH37gW9fxvecxQs81ekuk2KgAz8Oqp",
      "authorship_tag": "ABX9TyMjjfg056OUTLE58CLonuSz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cvjeongwon/MLP/blob/main/NLP_%EC%A0%84%EC%B2%98%EB%A6%AC%EC%8B%A4%EC%8A%B5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wz6XHQ6-2RBF",
        "outputId": "7f606cfd-2580-4d8d-e3a2-ee1723efcf10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Model-based', 'RL', 'do', \"n't\", 'need', 'a', 'value', 'function', 'for', 'the', 'ploicy', '.']\n"
          ]
        }
      ],
      "source": [
        "# 1. 표준 토큰화\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "\n",
        "tokenizer = TreebankWordTokenizer()\n",
        "text = \"Model-based RL don't need a value function for the ploicy.\"\n",
        "print(tokenizer.tokenize(text))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# \n",
        "from nltk.tokenize import word_tokenize\n",
        "# print(word_tokenize(text))"
      ],
      "metadata": {
        "id": "fqYQ5rLQ2jL8"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "from nltk.stem import PorterStemmer, LancasterStemmer\n",
        "stem1 = PorterStemmer()\n",
        "stem2 = LancasterStemmer()\n",
        "\n",
        "words=[\"eat\", \"ate\",\"eaten\", \"eating\"]\n",
        "print(\"PoterStemmer    :\", [stem1.stem(w) for w in words])\n",
        "print(\"LancasterStemmer:\", [stem2.stem(w) for w in words])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pIzSbQHw3r1Z",
        "outputId": "552d5138-569e-49ca-8dea-93d39f1d1cc0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PoterStemmer    : ['eat', 'ate', 'eaten', 'eat']\n",
            "LancasterStemmer: ['eat', 'at', 'eat', 'eat']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "lemm = WordNetLemmatizer()\n",
        "words=[\"eat\", \"ate\",\"eaten\", \"eating\"]\n",
        "print(\"WordNet Lemmatizer: \" ,[lemm.lemmatize(w,pos=\"v\")for w in words])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2G5v8XL4xIJ",
        "outputId": "393b386f-f7b7-4e6b-e72c-dc50a9e0bc73"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WordNet Lemmatizer:  ['eat', 'eat', 'eat', 'eat']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. StopWords 불용어\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "print(stopwords.words('english')[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jgVf1lFs5Qf5",
        "outputId": "a2d0f07b-b493-495a-a9c3-f8f745c1a3b0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install JPype1"
      ],
      "metadata": {
        "id": "Sl8JzvTl7CkT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install konlpy"
      ],
      "metadata": {
        "id": "T-c6hsKs7S0L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 토큰화 후 한글 불용어 제거하기\n",
        "# from konlpy.corpus import kobill\n",
        "from konlpy.tag import Okt\n",
        "\n",
        "okt = Okt()\n",
        "\n",
        "text =\"고기를 아무렇게나 구우려고 하면 안 돼. 고기라고 다 같은 게 아니거든. 예컨대 삼겹살을 구울 때는 중요한 게 있지.\"\n",
        "stop_words = \"를 아무렇게나 구 우려 고 안 돼 같은 게 구울 때 는\"\n",
        "\n",
        "stop_words = set(stop_words.split(' '))\n",
        "word_tokens = okt.morphs(text)\n",
        "result =[word for word in word_tokens if not word in stop_words]\n",
        "\n",
        "print(\"불용어 제거 전:\", word_tokens)\n",
        "print(\"불용어 제거 후:\", result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "estul1eq6HQZ",
        "outputId": "a76efa98-a1c8-4636-8ce5-a3d9a26ff1a8"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "불용어 제거 전: ['고기', '를', '아무렇게나', '구', '우려', '고', '하면', '안', '돼', '.', '고기', '라고', '다', '같은', '게', '아니거든', '.', '예컨대', '삼겹살', '을', '구울', '때', '는', '중요한', '게', '있지', '.']\n",
            "불용어 제거 후: ['고기', '하면', '.', '고기', '라고', '다', '아니거든', '.', '예컨대', '삼겹살', '을', '중요한', '있지', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Enumerate 예제\n",
        "mylist =['Apple', 'Tomato', 'Buleberry']\n",
        "for n, name in enumerate(mylist):\n",
        "  print(\"Fruit: {}, Number: {}\".format(name, n))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hgLDvd-E6sN5",
        "outputId": "86a48d47-9977-4b97-c5a0-b7716997b134"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fruit: Apple, Number: 0\n",
            "Fruit: Tomato, Number: 1\n",
            "Fruit: Buleberry, Number: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. 정수 인코딩 & Sorting\n",
        "vocab ={'apple':2, 'Tomato':6, 'Rice': 4, 'Blueberry': 1}\n",
        "vocab_sort = sorted(vocab.items(), key=lambda x: x[1], reverse=True)\n",
        "print(vocab_sort)\n",
        "word2inx = {word[0]: index+1 for index, word in enumerate(vocab_sort)}\n",
        "print(word2inx)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dM8OICJq-8EP",
        "outputId": "2f4121ce-6ec1-46cd-8771-a966a2304528"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Tomato', 6), ('Rice', 4), ('apple', 2), ('Blueberry', 1)]\n",
            "{'Tomato': 1, 'Rice': 2, 'apple': 3, 'Blueberry': 4}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. 코사인 Similarity\n",
        "import numpy as np\n",
        "def cos_sim(A,B):\n",
        "  return np.dot(A,B) / (np.linalg.norm(A)*np.linalg.norm(B))\n",
        "\n",
        "a = [1,0,0,1]\n",
        "b = [0,1,1,0]\n",
        "c = [1,1,1,1]\n",
        "print(cos_sim(a,b), cos_sim(b,c), cos_sim(c,a))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yYSXquuy_l8H",
        "outputId": "f4410edb-7486-44a9-ed41-4d9cf196b22e"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0 0.7071067811865475 0.7071067811865475\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Levenshtein distance](https://en.wikipedia.org/wiki/Levenshtein_distance)"
      ],
      "metadata": {
        "id": "mdnsRtqhDv8w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# text1과 text2 간의 레벤슈타인 거리 계산 함수\n",
        "def leven(text1, text2):\n",
        "    len1 = len(text1) + 1  # text1의 길이 + 1\n",
        "    len2 = len(text2) + 1  # text2의 길이 + 1\n",
        "    sim_array = np.zeros((len1, len2))  # 2차원 배열 초기화\n",
        "    sim_array[:,0] = np.linspace(0, len1-1, len1)  # 첫 번째 열 초기화\n",
        "    sim_array[0,:] = np.linspace(0, len2-1, len2)  # 첫 번째 행 초기화\n",
        "    for i in range(1,len1):\n",
        "        for j in range(1,len2):\n",
        "            add_char = sim_array[i-1,j] + 1  # 추가 연산 횟수\n",
        "            sub_char = sim_array[i,j-1] + 1  # 삭제 연산 횟수\n",
        "            if text1[i-1] == text2[j-1]:\n",
        "                mod_char = sim_array[i-1,j-1]  # 수정 연산 (동일한 문자), mod_char:이전 위치의 연산횟수\n",
        "            else:\n",
        "                mod_char = sim_array[i-1,j-1] + 1  # 수정 연산 (다른 문자), 이전위치에서의 연산횟수+1\n",
        "            sim_array[i,j] = min([add_char, sub_char, mod_char])  # 가장 작은 연산 횟수 선택\n",
        "    return sim_array[-1,-1]  # 최종 레벤슈타인 거리 반환, 2차원 배열의 마지막 요소"
      ],
      "metadata": {
        "id": "jNb6-GIaD0cF"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(leven('머신러닝','머신너닝')) # 최소수정횟수가 1\n",
        "print(leven('자연어 처리','자연어쩌리연산'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aXDv9sgFGDGI",
        "outputId": "35400e8f-c2a5-47c0-d012-334ec23f4815"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.0\n",
            "4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iSzq7TrwQN9J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# word2Vec p.173"
      ],
      "metadata": {
        "id": "yYamToVqWkKk"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Os2fv5_vWqpc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmHcWWg0Uiv-"
      },
      "source": [
        "## 4.1.3 Linear Regression Example with Word2Vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYAA9sxHUiwA"
      },
      "source": [
        "### Word2Vec Feature Example"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "tvImyyWnVvTz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "nfB8X8N0UiwA"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pHvHmqL3VGUW",
        "outputId": "631747cd-035d-4513-e0ee-27b8cfb42fcf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "VkAcBtfCUiwB"
      },
      "outputs": [],
      "source": [
        "DATA_IN_PATH = './data_in/'\n",
        "TRAIN_CLEAN_DATA = 'train_clean.csv'\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "TEST_SPLIT = 0.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        },
        "id": "SAKqbf1AUiwC",
        "outputId": "a870c922-e428-4aa0-dfda-da989fb37179"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-46-11f5b71d8e29>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_IN_PATH\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mTRAIN_CLEAN_DATA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1736\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    857\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data_in/train_clean.csv'"
          ]
        }
      ],
      "source": [
        "train_data = pd.read_csv(DATA_IN_PATH + TRAIN_CLEAN_DATA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "izde65jmUiwC"
      },
      "outputs": [],
      "source": [
        "reviews = list(train_data['review'])\n",
        "sentiments = list(train_data['sentiment'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HKH90ePBUiwC"
      },
      "outputs": [],
      "source": [
        "sentences = []\n",
        "for review in reviews:\n",
        "    sentences.append(review.split())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t2yZF6XFUiwD"
      },
      "outputs": [],
      "source": [
        "num_features = 300    \n",
        "min_word_count = 40   \n",
        "num_workers = 4       \n",
        "context = 10          \n",
        "downsampling = 1e-3 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oOzQzywQUiwD"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n",
        "   level=logging.INFO)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oIGpLg7_UiwD"
      },
      "outputs": [],
      "source": [
        "from gensim.models import word2vec\n",
        "\n",
        "model = word2vec.Word2Vec(sentences, workers=num_workers, \\\n",
        "           size=num_features, min_count = min_word_count, \\\n",
        "            window = context, sample = downsampling)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-df8rC9iUiwE"
      },
      "outputs": [],
      "source": [
        "def get_features(words, model, num_features):\n",
        "    feature_vector = np.zeros((num_features),dtype=np.float32)\n",
        "\n",
        "    num_words = 0\n",
        "    index2word_set = set(model.wv.index2word)\n",
        "\n",
        "    for w in words:\n",
        "        if w in index2word_set:\n",
        "            num_words += 1\n",
        "            feature_vector = np.add(feature_vector, model[w])\n",
        "\n",
        "    feature_vector = np.divide(feature_vector, num_words)\n",
        "    return feature_vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wpzoGnxgUiwE"
      },
      "outputs": [],
      "source": [
        "def get_dataset(reviews, model, num_features):\n",
        "    dataset = list()\n",
        "\n",
        "    for s in reviews:\n",
        "        dataset.append(get_features(s, model, num_features))\n",
        "\n",
        "    reviewFeatureVecs = np.stack(dataset)\n",
        "    \n",
        "    return reviewFeatureVecs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f9lp4h8oUiwF"
      },
      "outputs": [],
      "source": [
        "test_data_vecs = get_dataset(sentences, model, num_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uK9OYNtgUiwF"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "X = test_data_vecs\n",
        "y = np.array(sentiments)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SPLIT, random_state=RANDOM_SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e6ApZXSOUiwF"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "lgs = LogisticRegression(class_weight='balanced')\n",
        "lgs.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dslgBp7_UiwG"
      },
      "outputs": [],
      "source": [
        "print(\"Accuracy: %f\" % lgs.score(X_test, y_test)) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aLuzD1fEUiwG"
      },
      "outputs": [],
      "source": [
        "TEST_CLEAN_DATA = 'test_clean.csv'\n",
        "\n",
        "test_data = pd.read_csv(DATA_IN_PATH + TEST_CLEAN_DATA)\n",
        "\n",
        "test_review = list(test_data['review'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FC08wXaFUiwG"
      },
      "outputs": [],
      "source": [
        "test_data.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gZ3xSq2-UiwG"
      },
      "outputs": [],
      "source": [
        "test_sentences = list()\n",
        "for review in test_review:\n",
        "    test_sentences.append(review.split())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6fS0Q3hjUiwG"
      },
      "outputs": [],
      "source": [
        "test_data_vecs = get_dataset(test_sentences, model, num_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rOZr_y-BUiwH"
      },
      "outputs": [],
      "source": [
        "DATA_OUT_PATH = './data_out/'\n",
        "\n",
        "test_predicted = lgs.predict(test_data_vecs)\n",
        "\n",
        "if not os.path.exists(DATA_OUT_PATH):\n",
        "    os.makedirs(DATA_OUT_PATH)\n",
        "    \n",
        "ids = list(test_data['id'])\n",
        "answer_dataset = pd.DataFrame({'id': ids, 'sentiment': test_predicted})\n",
        "answer_dataset.to_csv(DATA_OUT_PATH + 'lgs_w2v_answer.csv', index=False, quoting=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fibnA99pUiwH"
      },
      "outputs": [],
      "source": [
        "model_name = \"300features_40minwords_10context\"\n",
        "model.save(model_name)"
      ]
    }
  ]
}